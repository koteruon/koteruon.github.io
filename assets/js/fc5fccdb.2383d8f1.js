"use strict";(self.webpackChunkblog=self.webpackChunkblog||[]).push([[2047],{15928:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/Table-Tennis-Video-Timeline-Diagram-a75d4fdaf2fff49cb7621228c3480913.png"},24080:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/Method-Flowchart-c67e2d12d1c7887ae2ce8bac9f6ec5df.png"},30143:(e,t,n)=>{n.d(t,{A:()=>r});n(96540);var s=n(13852),i=n(74848);const r=e=>{let{videoId:t,videoTitle:n,params:r}=e;return(0,i.jsx)("div",{children:(0,i.jsx)(s.A,{id:t,title:n,params:r})})}},46488:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/stroke-d5dce20845a9cbb756dabf6d64028ec4.png"},51433:(e,t,n)=>{n.d(t,{A:()=>o});n(96540);var s=n(58713),i=n.n(s),r=n(8532),a=n(74848);const o=e=>{let{username:t,repo:n}=e;const{isDarkTheme:s}=(0,r.G)();return(0,a.jsx)(i(),{username:t,repository:n,dark:s})}},62558:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/Network-Architecture-e27e25f5a9baaf6e855a322e1067b82b.png"},85353:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>m,frontMatter:()=>l,metadata:()=>s,toc:()=>h});const s=JSON.parse('{"id":"research/Table-Tennis-Referee-System-Using-Multimodal-Deep-Learning","title":"Table Tennis Referee System Using Multimodal Deep Learning","description":"---","source":"@site/docs/research/02-Table-Tennis-Referee-System-Using-Multimodal-Deep-Learning.mdx","sourceDirName":"research","slug":"/research/Table-Tennis-Referee-System-Using-Multimodal-Deep-Learning","permalink":"/docs/research/Table-Tennis-Referee-System-Using-Multimodal-Deep-Learning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"researchSidebar","previous":{"title":"Self Thesis","permalink":"/docs/research/self-thesis"},"next":{"title":"Table Tennis Trajectory Landing Point and Speed Analysis System","permalink":"/docs/research/Table-Tennis-Trajectory-Landing-Point-and-Speed-Analysis-System"}}');var i=n(74848),r=n(28453),a=n(51433),o=n(30143);const l={},d="Table Tennis Referee System Using Multimodal Deep Learning",c={},h=[{value:"Experimental Results",id:"experimental-results",level:2},{value:"Explanatory Videos",id:"explanatory-videos",level:2},{value:"Abstract",id:"abstract",level:2},{value:"Method Flowchart",id:"method-flowchart",level:2},{value:"Bimodal-based Action Recognition Model",id:"bimodal-based-action-recognition-model",level:2},{value:"Experimental Results",id:"experimental-results-1",level:2}];function p(e){const t={h1:"h1",h2:"h2",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",strong:"strong",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"table-tennis-referee-system-using-multimodal-deep-learning",children:"Table Tennis Referee System Using Multimodal Deep Learning"})}),"\n",(0,i.jsx)(a.A,{username:"koteruon",repo:"HIT"}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"experimental-results",children:"Experimental Results"}),"\n",(0,i.jsx)(o.A,{videoId:"vh6rItcCGlk",videoTitle:"\u57fa\u65bc\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u4e4b\u684c\u7403\u81ea\u52d5\u88c1\u5224\u7cfb\u7d71_112\u5e74\u5168\u570b\u904b\u52d5\u6703"}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"explanatory-videos",children:"Explanatory Videos"}),"\n",(0,i.jsx)(o.A,{videoId:"9BYXCaCJvdQ",videoTitle:"\u57fa\u65bc\u591a\u6a21\u614b\u6df1\u5ea6\u5b78\u7fd2\u4e4b\u684c\u7403\u81ea\u52d5\u88c1\u5224\u7cfb\u7d71"}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"abstract",children:"Abstract"}),"\n",(0,i.jsx)(t.p,{children:"This study pioneers the use of bimodal action recognition for table tennis video analysis, integrating RGB images and pose data to detect rally events.\nA YOLOv7-based framework with pose estimation distinguishes ball, player, and table positions, enhancing system speed and accuracy.\nBy combining ball trajectories with player actions, the system identifies rally start and end times, embedding results into output videos for easy analysis."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Table-Tennis-Video-Timeline-Diagram",src:n(15928).A+"",width:"3564",height:"920"})}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"method-flowchart",children:"Method Flowchart"}),"\n",(0,i.jsx)(t.p,{children:"The workflow of this study is as follows:"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsx)(t.li,{children:"First, object detection is performed on the input video to identify the ball, players, and table, along with human pose estimation.\nAn object detection model is used to predict the bounding boxes for each object, while pose estimation is applied to the players to detect key body joint coordinates."}),"\n",(0,i.jsx)(t.li,{children:"Next, a bimodal deep learning network is built using these two types of data to perform action recognition for each player.\nThe ball trajectory coordinates and the action recognition results are then used to determine the conclusion of a table tennis point."}),"\n",(0,i.jsx)(t.li,{children:"Finally, the results are integrated and displayed in the output video."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Method-Flowchart",src:n(24080).A+"",width:"3874",height:"2223"})}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"bimodal-based-action-recognition-model",children:"Bimodal-based Action Recognition Model"}),"\n",(0,i.jsxs)(t.p,{children:["This study aims to detect the start of each point in a table tennis match by identifying the ",(0,i.jsx)(t.strong,{children:"serve actions"}),", thereby enabling accurate recognition of players' continuous movements within each frame."]}),"\n",(0,i.jsxs)(t.p,{children:["Recent works have leveraged ",(0,i.jsx)(t.strong,{children:"spatiotemporal convolutional networks"})," to capture both ",(0,i.jsx)(t.strong,{children:"spatial"})," and ",(0,i.jsx)(t.strong,{children:"temporal motion features"})," in video frames, enhancing action recognition performance.\nHowever, these methods often overlook the ",(0,i.jsx)(t.strong,{children:"interactions between players and surrounding objects"}),", such as the ",(0,i.jsx)(t.strong,{children:"ball's movement across the table"}),", which is vital to the hitting process."]}),"\n",(0,i.jsxs)(t.p,{children:["To address this gap, we propose a ",(0,i.jsx)(t.strong,{children:"dual-modal approach"})," that incorporates both ",(0,i.jsx)(t.strong,{children:"player-environment interactions"})," and ",(0,i.jsx)(t.strong,{children:"human pose estimation"}),".\nThe model utilizes the ",(0,i.jsx)(t.strong,{children:"SlowFast network"})," for feature extraction, processing both ",(0,i.jsx)(t.strong,{children:"slow"})," and ",(0,i.jsx)(t.strong,{children:"fast temporal scales"})," to capture long- and short-term motion information.\n",(0,i.jsx)(t.strong,{children:"Region of Interest (RoI) alignment"})," is employed to focus on key objects and players by applying the detected bounding boxes.\nAdditionally, we introduce ",(0,i.jsx)(t.strong,{children:"interaction modules"}),"\u2014",(0,i.jsx)(t.strong,{children:"person-to-person"}),", ",(0,i.jsx)(t.strong,{children:"person-to-object"}),", and ",(0,i.jsx)(t.strong,{children:"hands-to-object interactions"}),"\u2014capturing complex relationships between the target objects.\nThese interactions are computed using a ",(0,i.jsx)(t.strong,{children:"cross-attention mechanism"}),", followed by ",(0,i.jsx)(t.strong,{children:"intra-modality aggregation"})," to refine the extracted features.\nFinally, ",(0,i.jsx)(t.strong,{children:"temporal interaction"})," and ",(0,i.jsx)(t.strong,{children:"feature fusion modules"})," are applied to integrate information across both time and modalities, resulting in improved action classification accuracy."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Network-Architecture",src:n(62558).A+"",width:"3163",height:"1980"})}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"experimental-results-1",children:"Experimental Results"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"serve",src:n(88075).A+"",width:"1600",height:"1200"})}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"stroke",src:n(46488).A+"",width:"3600",height:"1300"})})]})}function m(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},88075:(e,t,n)=>{n.d(t,{A:()=>s});const s=n.p+"assets/images/serve-5ae289f1c6c9b79cb8b56df8a19424a4.png"}}]);