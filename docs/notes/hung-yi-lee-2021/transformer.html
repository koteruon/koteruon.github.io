<!doctype html>
<html lang="zh-Hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notes/hung-yi-lee-2021/transformer" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">Transformer | Chao-En Huang</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://koteruon.github.io/images/icon/chao_en_huang_icon.png"><meta data-rh="true" name="twitter:image" content="https://koteruon.github.io/images/icon/chao_en_huang_icon.png"><meta data-rh="true" property="og:url" content="https://koteruon.github.io/docs/notes/hung-yi-lee-2021/transformer"><meta data-rh="true" property="og:locale" content="zh_Hant"><meta data-rh="true" name="docusaurus_locale" content="zh-Hant"><meta data-rh="true" name="docsearch:language" content="zh-Hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Transformer | Chao-En Huang"><meta data-rh="true" name="description" content="Transformer 與 Seq2seq 模型概論"><meta data-rh="true" property="og:description" content="Transformer 與 Seq2seq 模型概論"><link data-rh="true" rel="icon" href="/images/icon/favicon.ico"><link data-rh="true" rel="canonical" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2021/transformer"><link data-rh="true" rel="alternate" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2021/transformer" hreflang="zh-Hant"><link data-rh="true" rel="alternate" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2021/transformer" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Chao-En Huang RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Chao-En Huang Atom Feed">

<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-5X8N2TCV",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.42b2a4f7.css">
<script src="/assets/js/runtime~main.0cd360f8.js" defer="defer"></script>
<script src="/assets/js/main.d88531f0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5X8N2TCV" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Chao-En</b></a><a class="navbar__item navbar__link" href="/about-me">About Me</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/notes">Notes</a><a class="navbar__item navbar__link" href="/docs/research">Research</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/notes">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/notes/hung-yi-lee-2021/machine-learning-basics">李宏毅機器學習 2021</a><button aria-label="收起側邊欄分類 &#x27;李宏毅機器學習 2021&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/machine-learning-basics">機器學習基本概念簡介</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/machine-learning-task-guide">機器學習任務攻略</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/deep-learning-optimization-critical-point">深度學習中的優化技巧 - 臨界點 (Critical Point)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/deep-learning-optimization-batch-momentum">深度學習中的優化技巧 - 批次 (Batch) 與動量 (Momentum)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/deep-learning-optimization-adaptive-learning-rate">深度學習中的優化技巧 - 自動調整學習速率 (Adaptive Learning Rate)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/deep-learning-optimization-classification">深度學習中的優化技巧 - 分類 (Classification)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/deep-learning-optimization-batch-normalization">深度學習中的優化技巧 - 批次標準化 (Batch Normalization, BN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/cnn">卷積神經網路 (Convolutional Neural Network, CNN)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2021/self-attention">自注意力機制 (Self-attention)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/notes/hung-yi-lee-2021/transformer">Transformer</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai">李宏毅生成式AI 2024</a><button aria-label="展開側邊欄分類 &#x27;李宏毅生成式AI 2024&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2025/future-of-generative-ai">李宏毅生成式AI 2025</a><button aria-label="展開側邊欄分類 &#x27;李宏毅生成式AI 2025&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/design-pattern">Design Patterns</a><button aria-label="展開側邊欄分類 &#x27;Design Patterns&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/docker-basics">Docker Basics</a><button aria-label="展開側邊欄分類 &#x27;Docker Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/kubernetes-basics">Kubernetes Basics</a><button aria-label="展開側邊欄分類 &#x27;Kubernetes Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/owasp-top10-2021">OWASP Top 10 (2021)</a><button aria-label="展開側邊欄分類 &#x27;OWASP Top 10 (2021)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/ollama">Ollama</a><button aria-label="展開側邊欄分類 &#x27;Ollama&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/notes/hung-yi-lee-2021/machine-learning-basics"><span itemprop="name">李宏毅機器學習 2021</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Transformer</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>Transformer</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-與-seq2seq-模型概論">Transformer 與 Seq2seq 模型概論<a href="#transformer-與-seq2seq-模型概論" class="hash-link" aria-label="Transformer 與 Seq2seq 模型概論的直接連結" title="Transformer 與 Seq2seq 模型概論的直接連結">​</a></h2>
<p><strong>Transformer</strong> 本質上是一個 <strong>Sequence-to-sequence (Seq2seq)</strong> 模型，它與 BERT 有著強烈的關係。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="什麼是-seq2seq">什麼是 Seq2seq？<a href="#什麼是-seq2seq" class="hash-link" aria-label="什麼是 Seq2seq？的直接連結" title="什麼是 Seq2seq？的直接連結">​</a></h3>
<ul>
<li><strong>基本定義</strong>：輸入是一個序列 (Sequence)，輸出也是一個序列，但<strong>輸出長度由機器自行決定</strong>。</li>
<li><strong>核心組件</strong>：一般的 Seq2seq 模型分為 <strong>Encoder (編碼器)</strong> 與 <strong>Decoder (解碼器)</strong> 兩大部分。Encoder 負責處理輸入序列，並將結果交給 Decoder 決定最終輸出的序列。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="seq2seq" src="/assets/images/seq2seq-95c4b65558297e3c860403ff2904e257.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="seq2seq-的多元應用">Seq2seq 的多元應用<a href="#seq2seq-的多元應用" class="hash-link" aria-label="Seq2seq 的多元應用的直接連結" title="Seq2seq 的多元應用的直接連結">​</a></h3>
<p>Seq2seq 是一個非常強大的模型，其應用範圍極廣：</p>
<ul>
<li><strong>語音辨識</strong>：輸入聲音訊號（向量序列），輸出對應文字，長度由機器決定。</li>
<li><strong>機器翻譯</strong>：輸入一種語言的句子，輸出另一種語言。</li>
<li><strong>語音翻譯</strong>：直接將聲音訊號翻譯為另一種語言的文字，這對於「沒有文字」的語言（如部分台語應用場景）特別有用。</li>
<li><strong>聊天機器人</strong>：輸入對話文字，輸出回應文字。</li>
<li><strong>NLP 任務轉 QA</strong>：許多自然語言處理任務（如摘要、情感分析）都可以看作是「問答問題」，進而使用 Seq2seq 求解。</li>
<li><strong>文法剖析 (Syntactic Parsing)</strong>：將樹狀結構轉化為序列後，可用 Seq2seq 硬解。</li>
<li><strong>多標籤分類 (Multi-label Classification)</strong>：機器能自行決定一篇文章屬於多少個類別，解決傳統分類難以決定門檻值的問題。</li>
<li><strong>物件偵測 (Object Detection)</strong>：雖然看似不相關，但亦可使用 Seq2seq 的邏輯硬解。</li>
</ul>
<p><strong>注意</strong>：雖然 Seq2seq 像「瑞士刀」般萬用，但在特定任務（如語音辨識）中，客製化模型（如 RNN Transducer）往往能表現得更好。</p>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="seq2seq-1" src="/assets/images/seq2seq-1-74feb995a8d45d76cc42d691d18da692.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="text-to-speech" src="/assets/images/text-to-speech-62fdeba8677971e4cbd1634a48ab2f95.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="seq2seq-chatbot" src="/assets/images/seq2seq-chatbot-5f1a51194ab55f7626cb97a9d869df49.png" width="960" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>語音辨識、機器翻譯、語音翻譯</strong></td><td style="text-align:center"><strong>語音合成</strong></td><td style="text-align:center"><strong>聊天機器人</strong></td></tr></tbody></table>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="seq2seq-nlp" src="/assets/images/seq2seq-nlp-58045b6a93b4112b128a015286fbaf64.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="seq2seq-syntactic-parsing" src="/assets/images/seq2seq-syntactic-parsing-cd03111b6185a280711efec84d27b29c.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="seq2seq-multi-label-classification" src="/assets/images/seq2seq-multi-label-classification-5c82eec317a13b617d22ed5369771e3c.png" width="960" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>NLP 任務轉 QA</strong></td><td style="text-align:center"><strong>文法剖析</strong></td><td style="text-align:center"><strong>多標籤分類</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-encoder-架構解析">Transformer Encoder 架構解析<a href="#transformer-encoder-架構解析" class="hash-link" aria-label="Transformer Encoder 架構解析的直接連結" title="Transformer Encoder 架構解析的直接連結">​</a></h2>
<p>Encoder 的主要任務是：<strong>給入一排向量，輸出另外一排同樣長度的向量</strong>。在 Transformer 中，Encoder 由多個 <strong>Block</strong> 重複堆疊而成，每個 Block 並非單一層的神經網路，而是包含多個層級的複雜結構。</p>
<p><img decoding="async" loading="lazy" alt="encoder" src="/assets/images/encoder-30c581a2118367c28e9a7e73b23b05e2.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="encoder-block-的組成元素">Encoder Block 的組成元素<a href="#encoder-block-的組成元素" class="hash-link" aria-label="Encoder Block 的組成元素的直接連結" title="Encoder Block 的組成元素的直接連結">​</a></h3>
<p>每一個 Block 內部主要執行以下步驟：</p>
<ol>
<li><strong>多頭自注意力 (Multi-Head Self-attention)</strong>：考慮整個序列的資訊，輸出處理後的向量。</li>
<li><strong>殘差連接 (Residual Connection)</strong>：將 Self-attention 的輸出與原始輸入相加，得到新的結果。</li>
<li><strong>層正規化 (Layer Normalization)</strong>：<!-- -->
<ul>
<li>與 Batch Normalization 不同，<strong>Layer Norm 不需要考慮 Batch 資訊</strong>。</li>
<li>它是針對「同一個 example、同一個 feature」中不同的維度 (Dimension) 計算平均值 (Mean) 與標準差 (Standard Deviation)。</li>
</ul>
</li>
<li><strong>前饋神經網路 (Feed Forward Network)</strong>：通過全連接層 (Fully Connected Network) 進一步處理向量。</li>
<li><strong>第二次殘差連接與正規化</strong>：將前饋網路的輸入與輸出相加，再次進行 Layer Normalization 後輸出。</li>
</ol>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="encoder-block" src="/assets/images/encoder-block-936c4fc88ba1bf70209d5ca54d396672.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="residual-and-layer-normalization" src="/assets/images/residual-and-layer-normalization-596db83f79c07c9947b2039e76c7817e.png" width="960" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>每個 block 以 Self-attention 整合序列後，經 FC 輸出向量</strong></td><td style="text-align:center"><strong>Self-attention 與 FC 各自搭配 residual connection 與 layer normalization，逐步產生輸出向量</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-decoder-架構解析">Transformer Decoder 架構解析<a href="#transformer-decoder-架構解析" class="hash-link" aria-label="Transformer Decoder 架構解析的直接連結" title="Transformer Decoder 架構解析的直接連結">​</a></h2>
<p>Decoder 的主要任務是根據 Encoder 的輸出抽取出資訊，並產生最終的序列結果。目前最常見的是 <strong>Autoregressive (AT)</strong> 的運作方式：</p>
<ul>
<li><strong>逐字產生</strong>：Decoder 一次只產生一個字（Token），且前一個時間點的輸出會成為下一個時間點的輸入。</li>
<li><strong>起始符號 (BOS/BEGIN)</strong>：開始運作時，需給予一個特殊符號代表「開始」（如 <strong>BOS</strong> 或 <strong>BEGIN</strong>），通常以 One-Hot 向量表示。</li>
<li><strong>輸出機率分佈</strong>：Decoder 的輸出會經過 Softmax，得到一個與詞彙表（Vocabulary）等長的向量，代表每個字出現的機率。機率最高者即為該時間點的輸出。</li>
<li><strong>終止符號 (END)</strong>：為了讓機器知道何時停止，詞彙表中需加入一個特殊符號（如 <strong>END</strong> 或 <strong>斷</strong>）。當機器輸出此符號時，便結束生成過程。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="autoregressive-1" src="/assets/images/autoregressive-1-7260e1336dabaf95f280c0b54742f56e.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="autoregressive-2" src="/assets/images/autoregressive-2-f947ae9356dfb7d84dce47ed1e7e765d.png" width="960" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Decoder 以 Encoder 輸出與 <code>&lt;BOS&gt;</code> 為起點，逐步生成 token</strong></td><td style="text-align:center"><strong>逐步輸出 vocabulary 分佈，選擇最高機率的 token，重複直到產生 <code>&lt;EOS&gt;</code></strong></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="decoder-內部結構">Decoder 內部結構<a href="#decoder-內部結構" class="hash-link" aria-label="Decoder 內部結構的直接連結" title="Decoder 內部結構的直接連結">​</a></h3>
<p>Decoder 的結構與 Encoder 類似，同樣包含 Residual Connection 與 Layer Normalization，但有兩個關鍵差異：</p>
<p><img decoding="async" loading="lazy" alt="decoder" src="/assets/images/decoder-492432ec39efcbe09ab5d935039b4433.png" width="960" height="720" class="img_ev3q"></p>
<ol>
<li><strong>Masked Self-Attention</strong>：<!-- -->
<ul>
<li><strong>定義</strong>：這是一種特殊的自注意力機制，在產生輸出時，模型<strong>只能考慮左側（已產生）的資訊</strong>，不能看到右側（未產生）的資訊。</li>
<li><strong>原因</strong>：在實際產生序列時，未來的字尚未出現，因此無法被考慮。
<img decoding="async" loading="lazy" alt="masked-self-attention" src="/assets/images/masked-self-attention-2f068860f9e8ac57db95a49c95522f9d.png" width="960" height="720" class="img_ev3q"></li>
</ul>
</li>
<li><strong>Cross Attention (交叉注意力)</strong>：這是連接 Encoder 與 Decoder 的橋樑<!-- -->
<ul>
<li><strong>運作邏輯</strong>：Decoder 產生一個 <strong>Query (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi></mrow><annotation encoding="application/x-tex">q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">q</span></span></span></span>)</strong>，去 Encoder 輸出的 <strong>Key (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span>)</strong> 與 <strong>Value (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi></mrow><annotation encoding="application/x-tex">v</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.03588em">v</span></span></span></span>)</strong> 中抽取資訊。</li>
<li><strong>多層互動</strong>：在原始論文中，Decoder 的每一層都會去讀取 Encoder 最後一層的輸出，但研究指出這並非唯一連接方式。
<img decoding="async" loading="lazy" alt="cross-attention" src="/assets/images/cross-attention-723b4780e97822bf58ed268c73c2f57a.png" width="960" height="720" class="img_ev3q"></li>
</ul>
</li>
</ol>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="非自迴歸解碼器-non-autoregressive-nat">非自迴歸解碼器 (Non-Autoregressive, NAT)<a href="#非自迴歸解碼器-non-autoregressive-nat" class="hash-link" aria-label="非自迴歸解碼器 (Non-Autoregressive, NAT)的直接連結" title="非自迴歸解碼器 (Non-Autoregressive, NAT)的直接連結">​</a></h2>
<p>相對於 AT，<strong>NAT</strong> 一次產生整個序列：</p>
<ul>
<li><strong>優勢</strong>：<strong>平行化運算</strong>速度極快，且較能精準控制輸出長度（例如語音合成時的語速調整）。</li>
<li><strong>挑戰</strong>：效能通常不如 AT，且需要複雜的技巧（如處理 Multi-modality 問題）才能逼近 AT 的表現。</li>
<li><strong>長度預測</strong>：由於是一次產生，通常需另一個分類器來預測輸出的長度。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="at-vs-nat" src="/assets/images/at-vs-nat-d99b12015691995388a54c1ea1990c38.png" width="960" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="transformer-訓練過程">Transformer 訓練過程<a href="#transformer-訓練過程" class="hash-link" aria-label="Transformer 訓練過程的直接連結" title="Transformer 訓練過程的直接連結">​</a></h2>
<ul>
<li><strong>訓練資料</strong>：<!-- -->
<ul>
<li>需要大量的成對資料，例如「一段音頻」與其對應的「正確文字」（如工讀生聽打的結果）。</li>
<li>文字會被表示成 <strong>One-hot encoding</strong> 的向量，其中正確字所在的維度為 1，其餘為 0。.</li>
</ul>
</li>
<li><strong>訓練目標與流程</strong>：<!-- -->
<ul>
<li><strong>輸出分布</strong>：Decoder 的輸出會經過 Softmax 轉換成機率分佈（Distribution），其總和為 1。</li>
<li><strong>損失函數</strong>：每一次 Decoder 產生輸出時，就是在做一次分類問題。我們會計算輸出的機率分布與 <strong>Ground Truth</strong> 之間的 <strong>Cross Entropy</strong>。</li>
<li><strong>優化優化</strong>：透過最小化序列中所有位置（包含最終的「斷」符號）的 Cross Entropy 總和，實現梯度的優化，使模型輸出越接近正確答案越好。</li>
<li><strong>學習停頓</strong>：訓練時必須教導模型在正確位置輸出特殊符號「<strong>斷</strong>」(END)，代表語音辨識或翻譯任務已經結束。</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="training-process" src="/assets/images/training-process-db4665a881e5c8cafba606785c07601f.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="訓練核心挑戰與對策">訓練核心挑戰與對策<a href="#訓練核心挑戰與對策" class="hash-link" aria-label="訓練核心挑戰與對策的直接連結" title="訓練核心挑戰與對策的直接連結">​</a></h3>
<ul>
<li><strong>Teacher Forcing</strong>：
在訓練 Decoder 時，輸入的是<strong>正確答案</strong>（Ground Truth）而不是前一個時間點自己產生的答案。這能讓模型在學習初期更有效地對準正確目標。</li>
<li><strong>暴露偏差 (Exposure Bias)</strong>：
Teacher Forcing 導致訓練與測試（Inference）不一致。測試時 Decoder 看到的是自己產生的錯誤輸入，這可能導致「一步錯，步步錯」。</li>
<li><strong>解決方案 (Scheduled Sampling)</strong>：
為了修正此問題，可以在訓練中適度混入錯誤的答案（Scheduled Sampling），讓模型學習如何應對非正確的輸入，增加強健性。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="teacher-forcing" src="/assets/images/teacher-forcing-2d74ee22b99725dc647c3beeab375bb1.png" width="960" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="scheduled-sampling" src="/assets/images/scheduled-sampling-23bb938f7aceff3bb2309945888be7f0.png" width="960" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Teacher Forcing</strong></td><td style="text-align:center"><strong>Scheduled Sampling</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="訓練技巧">訓練技巧<a href="#訓練技巧" class="hash-link" aria-label="訓練技巧的直接連結" title="訓練技巧的直接連結">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="複製機制-copy-mechanism">複製機制 (Copy Mechanism)<a href="#複製機制-copy-mechanism" class="hash-link" aria-label="複製機制 (Copy Mechanism)的直接連結" title="複製機制 (Copy Mechanism)的直接連結">​</a></h3>
<ul>
<li>對於聊天機器人或摘要任務，模型不需「創造」新詞，而是從輸入中直接<strong>複製</strong>特定詞彙（如人名）。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="copy-mechanism" src="/assets/images/copy-mechanism-a502bb09e240d2c08dcf79cca5879df2.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="導引式注意力-guided-attention">導引式注意力 (Guided Attention)<a href="#導引式注意力-guided-attention" class="hash-link" aria-label="導引式注意力 (Guided Attention)的直接連結" title="導引式注意力 (Guided Attention)的直接連結">​</a></h3>
<ul>
<li>在語音合成或辨識中，要求 Attention 必須呈現固定的<strong>由左向右</strong>線性關係，避免漏字或重複。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="guided-attention" src="/assets/images/guided-attention-4d08ae6a16f4f3fe2b3b0e3b53e90b9f.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="束搜尋-beam-search">束搜尋 (Beam Search)<a href="#束搜尋-beam-search" class="hash-link" aria-label="束搜尋 (Beam Search)的直接連結" title="束搜尋 (Beam Search)的直接連結">​</a></h3>
<ul>
<li><strong>Greedy Decoding</strong> 每次只找機率最高的字，但不一定能得到全局最優序列。</li>
<li><strong>Beam Search</strong> 會保留數條機率較高的路徑進行搜尋。</li>
<li><strong>注意</strong>：在需要創造力的任務（如故事生成）或語音合成中，有時加入<strong>隨機性 (Noise)</strong> 反而比尋找最高分路徑效果更好。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="beam-search" src="/assets/images/beam-search-d499cd48404a3a8e7a232431b37a6e1f.png" width="960" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="優化目標cross-entropy-vs-bleu-score">優化目標：Cross Entropy vs. BLEU Score<a href="#優化目標cross-entropy-vs-bleu-score" class="hash-link" aria-label="優化目標：Cross Entropy vs. BLEU Score的直接連結" title="優化目標：Cross Entropy vs. BLEU Score的直接連結">​</a></h3>
<ul>
<li>訓練時通常最小化 <strong>Cross Entropy</strong>（逐字計算），但評估標準通常是 <strong>BLEU Score</strong>（整句比較）。</li>
<li>由於 BLEU Score 無法微分，若要直接優化 BLEU Score，通常需要使用<strong>強化學習 (RL)</strong> 技術。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="cross-entropy-vs-bleu-score" src="/assets/images/cross-entropy-vs-bleu-score-ae5a1807d0566cc77c0dcac754424f91.png" width="960" height="720" class="img_ev3q"></p></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/notes/hung-yi-lee-2021/self-attention"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">自注意力機制 (Self-attention)</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">生成式 AI 是什麼？</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#transformer-與-seq2seq-模型概論" class="table-of-contents__link toc-highlight">Transformer 與 Seq2seq 模型概論</a><ul><li><a href="#什麼是-seq2seq" class="table-of-contents__link toc-highlight">什麼是 Seq2seq？</a></li><li><a href="#seq2seq-的多元應用" class="table-of-contents__link toc-highlight">Seq2seq 的多元應用</a></li></ul></li><li><a href="#transformer-encoder-架構解析" class="table-of-contents__link toc-highlight">Transformer Encoder 架構解析</a><ul><li><a href="#encoder-block-的組成元素" class="table-of-contents__link toc-highlight">Encoder Block 的組成元素</a></li></ul></li><li><a href="#transformer-decoder-架構解析" class="table-of-contents__link toc-highlight">Transformer Decoder 架構解析</a><ul><li><a href="#decoder-內部結構" class="table-of-contents__link toc-highlight">Decoder 內部結構</a></li></ul></li><li><a href="#非自迴歸解碼器-non-autoregressive-nat" class="table-of-contents__link toc-highlight">非自迴歸解碼器 (Non-Autoregressive, NAT)</a></li><li><a href="#transformer-訓練過程" class="table-of-contents__link toc-highlight">Transformer 訓練過程</a><ul><li><a href="#訓練核心挑戰與對策" class="table-of-contents__link toc-highlight">訓練核心挑戰與對策</a></li></ul></li><li><a href="#訓練技巧" class="table-of-contents__link toc-highlight">訓練技巧</a><ul><li><a href="#複製機制-copy-mechanism" class="table-of-contents__link toc-highlight">複製機制 (Copy Mechanism)</a></li><li><a href="#導引式注意力-guided-attention" class="table-of-contents__link toc-highlight">導引式注意力 (Guided Attention)</a></li><li><a href="#束搜尋-beam-search" class="table-of-contents__link toc-highlight">束搜尋 (Beam Search)</a></li><li><a href="#優化目標cross-entropy-vs-bleu-score" class="table-of-contents__link toc-highlight">優化目標：Cross Entropy vs. BLEU Score</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">This Website</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/notes">Notes</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/research">Research</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://profile.104.com.tw/profile/9f11e5a7-4d3b-4a21-9241-ba6e3f9003c8/about" target="_blank" rel="noopener noreferrer" class="footer__link-item">104<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/%E7%85%A7%E6%81%A9-%E9%BB%83-93511b25b" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Acknowledgement</div><ul class="footer__items clean-list"><li class="footer__item">
              <p>
              illustrations by <a href="https://storyset.com/web">Storyset</a>
              </p>
              </li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Chao-En Huang. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>