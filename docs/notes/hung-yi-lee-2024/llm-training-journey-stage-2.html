<!doctype html>
<html lang="zh-Hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notes/hung-yi-lee-2024/llm-training-journey-stage-2" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">大型語言模型修練史 — 第二階段：名師指點，發揮潛力 | Chao-En Huang</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://koteruon.github.io/images/icon/chao_en_huang_icon.png"><meta data-rh="true" name="twitter:image" content="https://koteruon.github.io/images/icon/chao_en_huang_icon.png"><meta data-rh="true" property="og:url" content="https://koteruon.github.io/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-2"><meta data-rh="true" property="og:locale" content="zh_Hant"><meta data-rh="true" name="docusaurus_locale" content="zh-Hant"><meta data-rh="true" name="docsearch:language" content="zh-Hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="大型語言模型修練史 — 第二階段：名師指點，發揮潛力 | Chao-En Huang"><meta data-rh="true" name="description" content="在第一階段，模型透過自我學習累積了深厚的「內功」（世界知識與語言知識），但仍不知道如何正確回答問題，因此需要進入第二階段：指令微調 (Instruction Fine-tuning)。"><meta data-rh="true" property="og:description" content="在第一階段，模型透過自我學習累積了深厚的「內功」（世界知識與語言知識），但仍不知道如何正確回答問題，因此需要進入第二階段：指令微調 (Instruction Fine-tuning)。"><link data-rh="true" rel="icon" href="/images/icon/favicon.ico"><link data-rh="true" rel="canonical" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-2"><link data-rh="true" rel="alternate" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-2" hreflang="zh-Hant"><link data-rh="true" rel="alternate" href="https://koteruon.github.io/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-2" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Chao-En Huang RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Chao-En Huang Atom Feed">

<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-5X8N2TCV",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.42b2a4f7.css">
<script src="/assets/js/runtime~main.cc07911e.js" defer="defer"></script>
<script src="/assets/js/main.248e8ac7.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5X8N2TCV" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Chao-En</b></a><a class="navbar__item navbar__link" href="/about-me">About Me</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/notes">Notes</a><a class="navbar__item navbar__link" href="/docs/research">Research</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/notes">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2021/machine-learning-basics">李宏毅機器學習 2021</a><button aria-label="展開側邊欄分類 &#x27;李宏毅機器學習 2021&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai">李宏毅生成式AI 2024</a><button aria-label="收起側邊欄分類 &#x27;李宏毅生成式AI 2024&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai">生成式 AI 是什麼？</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2024/why-generative-ai-is-powerful">今日的生成式 AI 厲害在哪裡？</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2024/you-can-train-yourself">訓練不了人工智慧？你可以訓練你自己</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-1">大型語言模型修練史 — 第一階段：自我學習，累積實力</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-2">大型語言模型修練史 — 第二階段：名師指點，發揮潛力</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-3">大型語言模型修練史 — 第三階段：參與實戰，打磨技巧</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2025/future-of-generative-ai">李宏毅生成式AI 2025</a><button aria-label="展開側邊欄分類 &#x27;李宏毅生成式AI 2025&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/design-pattern">Design Patterns</a><button aria-label="展開側邊欄分類 &#x27;Design Patterns&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/docker-basics">Docker Basics</a><button aria-label="展開側邊欄分類 &#x27;Docker Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/kubernetes-basics">Kubernetes Basics</a><button aria-label="展開側邊欄分類 &#x27;Kubernetes Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/owasp-top10-2021">OWASP Top 10 (2021)</a><button aria-label="展開側邊欄分類 &#x27;OWASP Top 10 (2021)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/ollama">Ollama</a><button aria-label="展開側邊欄分類 &#x27;Ollama&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai"><span itemprop="name">李宏毅生成式AI 2024</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">大型語言模型修練史 — 第二階段：名師指點，發揮潛力</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>大型語言模型修練史 — 第二階段：名師指點，發揮潛力</h1></header>
<p>在第一階段，模型透過自我學習累積了深厚的「內功」（世界知識與語言知識），但仍不知道如何正確回答問題，因此需要進入第二階段：<strong>指令微調 (Instruction Fine-tuning)</strong>。</p>
<p><img decoding="async" loading="lazy" alt="llm-training-journey-stage-2" src="/assets/images/llm-training-journey-stage-2-953fbc6748b3f8528173f65c0b4961b7.png" width="1278" height="720" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="訓練機制指令微調-instruction-fine-tuning">訓練機制：指令微調 (Instruction Fine-tuning)<a href="#訓練機制指令微調-instruction-fine-tuning" class="hash-link" aria-label="訓練機制：指令微調 (Instruction Fine-tuning)的直接連結" title="訓練機制：指令微調 (Instruction Fine-tuning)的直接連結">​</a></h2>
<ul>
<li><strong>教材格式</strong>：人類老師需要準備「問題」與「正確答案」的成對資料。</li>
<li><strong>文字接龍化</strong>：將問答轉換成模型可理解的格式，例如「使用者：[問題] AI：[答案]」。模型學習在看到「AI：」符號後，接出正確的答案。</li>
<li><strong>標註的重要性</strong>：資料必須清楚標明哪個部分是<strong>使用者 (User)</strong> 講的，哪個部分是 <strong>AI</strong> 講的。<!-- -->
<ul>
<li>若未標註，模型會無法區分這是在對話還是在自問自答，導致輸出的結果不正確。</li>
<li>即使在 ChatGPT 介面上看不到，背後的文字接龍過程很可能也包含這些代表身分的符號。</li>
</ul>
</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="instruction-fine-tuning" src="/assets/images/instruction-fine-tuning-62db16b07c1f293976f6f1c376f14dac.png" width="1278" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="importance-of-annotation" src="/assets/images/importance-of-annotation-7b73fff5e45939e230bd5dad76b7aca5.png" width="1278" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>人類老師的指令微調</strong></td><td style="text-align:center"><strong>需要標註使用者和 AI</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="為什麼需要預訓練-pre-train-作為基礎">為什麼需要預訓練 (Pre-train) 作為基礎？<a href="#為什麼需要預訓練-pre-train-作為基礎" class="hash-link" aria-label="為什麼需要預訓練 (Pre-train) 作為基礎？的直接連結" title="為什麼需要預訓練 (Pre-train) 作為基礎？的直接連結">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="人類標註資料的限制與錯誤學習風險">人類標註資料的限制與錯誤學習風險<a href="#人類標註資料的限制與錯誤學習風險" class="hash-link" aria-label="人類標註資料的限制與錯誤學習風險的直接連結" title="人類標註資料的限制與錯誤學習風險的直接連結">​</a></h3>
<ul>
<li><strong>資料稀缺性</strong>：相較於網路上近乎無窮的文字，人類能提供的標註資料（問題與正確答案）非常有限。</li>
<li><strong>機器的偷懶特性</strong>：機器學習的唯一目標是「找到一組滿足訓練資料要求的參數」，它並不像人類一樣理解邏輯。</li>
<li><strong>錯誤規則的風險</strong>：<!-- -->
<ul>
<li><strong>實例</strong>：若教材只教「臺灣最高的山是哪座？答案：玉山」，機器可能會找到一組參數，其規則是「<strong>只要看到『最』這個字，就輸出『玉山』</strong>」。這組參數完全符合訓練要求，但若問到「世界最深的海溝在哪裡？」，模型會因為看到「最」字而同樣回答「玉山」。</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="data-scarcity" src="/assets/images/data-scarcity-37b4f2c0372d3108f3ffd7d22831cb54.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="初始參數的決定性影響">初始參數的決定性影響<a href="#初始參數的決定性影響" class="hash-link" aria-label="初始參數的決定性影響的直接連結" title="初始參數的決定性影響的直接連結">​</a></h3>
<ul>
<li><strong>繼承強大內功</strong>：成功關鍵在於將第一階段透過海量資料學到的參數（如 GPT-3 或 PaLM 的參數）作為第二階段的<strong>初始參數</strong>。這意味著最佳化的過程是從一個已經具備「上乘內功」的起點開始找尋。</li>
<li><strong>微調 (Fine-tune) 的定義</strong>：因為初始參數已經具備複雜且有道理的規則，在第二階段進行微調時，找出的新參數不會與初始參數偏離太多，從而確保模型行為仍然維持在有道理的範圍內。
<img decoding="async" loading="lazy" alt="the-impact-of-initialization" src="/assets/images/the-impact-of-initialization-98addbfdbc56d8dfb4b327222066b03b.png" width="1278" height="720" class="img_ev3q"></li>
<li><strong>傳承複雜的規則</strong>：<!-- -->
<ul>
<li>預訓練模型看過天文數字般的資料，它必須學會極其<strong>複雜的規則</strong>才能成功預測下一個字（例如它知道臺灣最高接玉山、世界最高接聖母峰），而不只是看關鍵字。</li>
<li>由於初始參數本身就蘊含了這些正確的邏輯，微調後的模型就不容易退化成那種「看到『最』就接『玉山』」的無腦規則。
<img decoding="async" loading="lazy" alt="inheritance-of-complex-rules" src="/assets/images/inheritance-of-complex-rules-49bed2bfee4e667d8624ba329ccf7865.png" width="1278" height="720" class="img_ev3q"></li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="極致的舉一反三能力">極致的「舉一 反三」能力<a href="#極致的舉一反三能力" class="hash-link" aria-label="極致的「舉一反三」能力的直接連結" title="極致的「舉一反三」能力的直接連結">​</a></h3>
<ul>
<li><strong>裸考實測案例</strong>：<!-- -->
<ul>
<li><strong>Multilingual BERT</strong>：在 104 種語言上預訓練過。若只在「英文」的閱讀理解資料上進行微調（教它如何做英文考題），它會自動學會如何做「中文」的閱讀理解。</li>
<li><strong>實驗數據</strong>：測試一組完全沒看過中文考題的模型（對中文而言是「<strong>裸考</strong>」），其正確率高達 <strong>78%</strong>，竟然與直接拿中文標註資料訓練出的模型表現相當。</li>
</ul>
</li>
<li><strong>原理總結</strong>：因為預訓練時模型已經具備了「世界最高是聖母峰」等知識，微調只是教它「<strong>回答問題的方法</strong>」。一旦它學會了答題框架，就能自動套用到所有它已具備知識的語言中。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="extreme-generalization" src="/assets/images/extreme-generalization-848b15cc22f5b00fe9e98caf92c25b0b.png" width="1278" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="技術細節adapter-與-lora">技術細節：Adapter 與 LoRA<a href="#技術細節adapter-與-lora" class="hash-link" aria-label="技術細節：Adapter 與 LoRA的直接連結" title="技術細節：Adapter 與 LoRA的直接連結">​</a></h2>
<ul>
<li><strong>Adapter 概念</strong>：在做微調時，完全不動預訓練的初始參數，僅在原函式後增加少量的未知數進行最佳化。</li>
<li><strong>LoRA</strong>：是 Adapter 的一種，透過這種方式可以確保新參數與舊參數非常類似。</li>
<li><strong>優點</strong>：<!-- -->
<ul>
<li><strong>減少運算量</strong>：不需要找尋所有參數，適合資源有限的環境（如 Colab 免費版）。</li>
<li><strong>維持效能</strong>：在不動原始大腦的情況下，加入少量新參數即可達成目標。</li>
</ul>
</li>
</ul>
<p><img decoding="async" loading="lazy" alt="lora" src="/assets/images/lora-0f14974a318a4b86b990a066fe530a2f.png" width="1278" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="微調的兩條路線打造專才-specialist">微調的兩條路線：打造專才 (Specialist)<a href="#微調的兩條路線打造專才-specialist" class="hash-link" aria-label="微調的兩條路線：打造專才 (Specialist)的直接連結" title="微調的兩條路線：打造專才 (Specialist)的直接連結">​</a></h2>
<p>針對特定任務（如翻譯、編修）收集大量標註資料，打造只會該項技能的模型。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="具體實例說明">具體實例說明<a href="#具體實例說明" class="hash-link" aria-label="具體實例說明的直接連結" title="具體實例說明的直接連結">​</a></h3>
<ul>
<li><strong>翻譯專才</strong>：收集大量的翻譯對資料（例如：輸入 &quot;Good morning&quot;，正確輸出為「早安」；輸入 &quot;How are you&quot;，輸出「你好嗎」）。</li>
<li><strong>編修專才</strong>：收集文法錯誤與修正後的對應資料。例如當使用者輸入 &quot;who care&quot; 時，模型會學習輸出應加上 s 變成 &quot;who cares&quot;。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="specialist" src="/assets/images/specialist-232b894983d90d77cf8a13075445e1b7.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="代表性模型與歷史背景">代表性模型與歷史背景<a href="#代表性模型與歷史背景" class="hash-link" aria-label="代表性模型與歷史背景的直接連結" title="代表性模型與歷史背景的直接連結">​</a></h3>
<ul>
<li><strong>BERT 系列</strong>：BERT 模型是走專才路線的典型代表，過去研究者常以 BERT 為基礎，透過  微調打造出應對各種不同任務的專才系統。</li>
<li><strong>早期 GPT 應用</strong>：在大型語言模型技術發展初期，人們使用 GPT 的方式也是拿它來打造一堆專門處理特定任務的專才系統。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="specialist-bert-series" src="/assets/images/specialist-bert-series-0d8758f5b2919e2fd069a6a521a0e008.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="效能表現">效能表現<a href="#效能表現" class="hash-link" aria-label="效能表現的直接連結" title="效能表現的直接連結">​</a></h3>
<ul>
<li><strong>超越人類平均</strong>：在 2019 年的研究中，研究者針對 9 個不同的任務分別打造了專才模型。實驗數據顯示，這些以強大模型為基礎打造出的專才，其平均能力在某些任務上甚至能超過人類的表現。</li>
<li><strong>開發成本</strong>：雖然專才表現強悍，但缺點在於世界上任務成千上萬，為每一項任務都單獨收集資料並訓練一個專屬模型顯得過於麻煩，這也促使了後來「通才」路線的興起。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="specialist-performance" src="/assets/images/specialist-performance-50d1369407a9e1b7404eb0330503b218.png" width="1278" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="微調的兩條路線打造通才-generalist">微調的兩條路線：打造通才 (Generalist)<a href="#微調的兩條路線打造通才-generalist" class="hash-link" aria-label="微調的兩條路線：打造通才 (Generalist)的直接連結" title="微調的兩條路線：打造通才 (Generalist)的直接連結">​</a></h2>
<p>將所有想得到的任務標註資料全部集合起來交給模型。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="具體實例說明-1">具體實例說明<a href="#具體實例說明-1" class="hash-link" aria-label="具體實例說明的  直接連結" title="具體實例說明的直接連結">​</a></h3>
<ul>
<li><strong>追求舉一反三的能力</strong>：期望模型不僅能完成教過的任務，還能處理沒看過的「任務變形」。例如在訓練中只分開教過「翻譯」與「摘要」，但通才模型在面對「請把這篇文章摘要後並翻譯」這類複合式新任務時，也能正確執行。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="generalist" src="/assets/images/generalist-eb066854683b533df37b2d7fb38a9125.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="技術挑戰連續學習與遺忘">技術挑戰：連續學習與遺忘<a href="#技術挑戰連續學習與遺忘" class="hash-link" aria-label="技術挑戰：連續學習與遺忘的直接連結" title="技術挑戰：連續學習與遺忘的直接連結">​</a></h3>
<ul>
<li><strong>災難性遺忘</strong>：19 年的研究（如以 GPT-2 為基礎的實驗）發現，若一個個任務連續教給語言模型，模型可能會忘記舊任務。</li>
<li><strong>解決方案</strong>：研究者開發了讓模型在腦中「複習」已學過知識的方法，以維持多樣化的任務處理能力。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="generalist-catastrophic-forgetting" src="/assets/images/generalist-catastrophic-forgetting-431b5afca7052fb96cd1ca179ca98488.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="關鍵進展google-的-flan-系列">關鍵進展：Google 的 FLAN 系列<a href="#關鍵進展google-的-flan-系列" class="hash-link" aria-label="關鍵進展：Google 的 FLAN 系列的直接連結" title="關鍵進展：Google 的 FLAN 系列的直接連結">​</a></h3>
<ul>
<li><strong>FLAN (Fine-tuned Language Net)</strong>：Google 在 2021 年證實，在大量任務上做過指令微調後，模型在「從未看過的新任務」上的表現能顯著超越原始的 GPT-3。</li>
<li><strong>規模 化 (Scaling) 的效益</strong>：2022 年的研究進一步將任務增加到 <strong>1800 個</strong>。這裡指的不是 1800 筆資料，而是 1800 種不同的任務（如翻譯是一個任務，裡面包含上萬筆資料）。</li>
<li><strong>研究發現</strong>：無論模型大小，隨著訓練的任務數量越多（資料集多樣性越高），模型在陌生任務上的表現就越好。特別是<strong>小模型</strong>在接受指令微調後，能力的提升尤其顯著。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="generalist-flan" src="/assets/images/generalist-flan-d4dd4507b724508c86551fc4da69d75d.png" width="1278" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="generalist-task-diversity" src="/assets/images/generalist-task-diversity-9c61ecd0aa1f0418efdec6678f1ffbc8.png" width="1278" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>FLAN (Fine-tuned Language Net)</strong></td><td style="text-align:center"><strong>不論模型大小，任務越多表現越好</strong></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="openai-的突破instructgpt-與真實數據">OpenAI 的突破：InstructGPT 與真實數據<a href="#openai-的突破instructgpt-與真實數據" class="hash-link" aria-label="OpenAI 的突破：InstructGPT 與真實數據的直接連結" title="OpenAI 的突破：InstructGPT 與真實數據的直接連結">​</a></h3>
<ul>
<li><strong>超越死板模板</strong>：Google 的 FLAN 資料多由研究人員使用「固定模板」將現成資料集轉化為問答。OpenAI 則認為這種方式與人類真實的使用情境落差太大。</li>
<li><strong>真實使用者數據</strong>：OpenAI 利用 GPT-3 線上服務累積的真實對話，發現使用者問的問題千變萬化（如寫故事、寫遺囑、提供職涯規劃建議等），遠比學術模板多樣化。</li>
<li><strong>實測結果</strong>：使用真實使用者數據微調出的 <strong>InstructGPT</strong>，在人類喜好程度上顯著勝過原本的 GPT-3 以及 Google 的 FLAN。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="generalist-instructgpt-qa-template" src="/assets/images/generalist-instructgpt-qa-template-e4e3c40c252a7e709263e09f42f210da.png" width="1278" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="generalist-instructgpt-results" src="/assets/images/generalist-instructgpt-results-37fd6455e991e51ae07097623c8c184e.png" width="1278" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>透過真實使用者的問答模板改進訓練模板</strong></td><td style="text-align:center"><strong>實測結果顯示 InstructGPT 在人類喜好度上勝過 GPT-3 與 FLAN</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="高品質資料的匱乏與-self-instruct-的誕生">高品質資料的匱乏與 Self-Instruct 的誕生<a href="#高品質資料的匱乏與-self-instruct-的誕生" class="hash-link" aria-label="高品質資料的匱乏與 Self-Instruct 的誕生的直接連結" title="高品質資料的匱乏與 Self-Instruct 的誕生的直接連結">​</a></h2>
<p>一般人不像 OpenAI 擁有線上系統可以收集真實使用者的千變萬化問題（如腦力激盪、寫故事等）。</p>
<p><img decoding="async" loading="lazy" alt="lack-of-high-quality-instructions" src="/assets/images/lack-of-high-quality-instructions-c899e1971b9f7c034bf4a6f929e11fd9.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="逆向工程self-instruct">逆向工程（Self-Instruct）<a href="#逆向工程self-instruct" class="hash-link" aria-label="逆向工程（Self-Instruct）的直接連結" title="逆向工程（Self-Instruct）的直接連結">​</a></h3>
<ul>
<li><strong>以 ChatGPT 為師</strong>：既然沒有資料，就透過逆向工程對 ChatGPT 進行「Self-Instruct」。</li>
<li><strong>三步驟流程</strong>：1. <strong>發想任務</strong>：問 ChatGPT 模型通常能做什麼事（如寫郵件、摘要）。2. <strong>幻想問題</strong>：請它針對任務幻想使用者可能會有的各種輸入問題。3. <strong>產出答案</strong>：將幻想出的問題丟回給 ChatGPT 產出正確答案，以此自動生成訓練用的標註資料。</li>
<li><strong>結果</strong>：小團隊雖然沒有真實數據，但能透過此方式取得「沒魚蝦也好」的訓練教材，開啟微調的第一步。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="self-instruct" src="/assets/images/self-instruct-1b3fb98a9f60152828b0d8a9252fa87e.png" width="1278" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="預訓練模型參數的壟斷與-llama-的開放">預訓練模型參數的壟斷與 LLaMA 的開放<a href="#預訓練模型參數的壟斷與-llama-的開放" class="hash-link" aria-label="預訓練模型參數的壟斷與 LLaMA 的開放的直接連結" title="預訓練模型參數的壟斷與 LLaMA 的開放的直接連結">​</a></h2>
<p>強大的預訓練模型（如 GPT-3 或 PaLM）參數並不公開，這導致全世界曾一度卡在「沒人能打造自己模型」的狀態。</p>
<p><img decoding="async" loading="lazy" alt="lack-of-pretrained-models" src="/assets/images/lack-of-pretrained-models-406328e1cf0741ff10f75219f93fa085.png" width="1278" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="llama-的出現與解放">LLaMA 的出現與解放<a href="#llama-的出現與解放" class="hash-link" aria-label="LLaMA 的出現與解放的直接連結" title="LLaMA 的出現與解放的直接連結">​</a></h3>
<ul>
<li><strong>Meta 改變了生態</strong>：2023 年 2 月 Meta 釋出了 LLaMA 的預訓練參數，打破了技術壟斷，讓任何人都能以  其為初始參數來打造模型。</li>
<li><strong>子孫模型的爆發</strong>：<!-- -->
<ul>
<li><strong>Alpaca (Stanford)</strong>：使用 LLaMA 參數搭配從 ChatGPT 取得的 5 萬筆資料微調而成。</li>
<li><strong>Vicuna</strong>：使用 LLaMA 參數搭配分享網站上的 7 萬筆真實對話資料微調而成。</li>
</ul>
</li>
<li><strong>結果</strong>：結合 <strong>LLaMA</strong> 與 <strong>Adapter (如 LoRA)</strong> 技術，大幅減少了所需的運算量，讓「人人都可以 Fine-tune 大型語言模型」的時代正式開啟。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="llama" src="/assets/images/llama-6ffb11922b8de7ead1839242aa7fac3e.png" width="1278" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="the-age-of-llms" src="/assets/images/the-age-of-llms-20fa65f20e5c6be54a0e33c61bdf830f.png" width="1278" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Meta 釋出了 LLaMA 的預訓練參數</strong></td><td style="text-align:center"><strong>大語言模型時代開始了</strong></td></tr></tbody></table></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-1"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">大型語言模型修練史 — 第一階段：自我學習，累積實力</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/notes/hung-yi-lee-2024/llm-training-journey-stage-3"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">大型語言模型修練史 — 第三階段：參與實戰，打磨技巧</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#訓練機制指令微調-instruction-fine-tuning" class="table-of-contents__link toc-highlight">訓練機制：指令微調 (Instruction Fine-tuning)</a></li><li><a href="#為什麼需要預訓練-pre-train-作為基礎" class="table-of-contents__link toc-highlight">為什麼需要預訓練 (Pre-train) 作為基礎？</a><ul><li><a href="#人類標註資料的限制與錯誤學習風險" class="table-of-contents__link toc-highlight">人類標註資料的限制與錯誤學習風險</a></li><li><a href="#初始參數的決定性影響" class="table-of-contents__link toc-highlight">初始參數的決定性影響</a></li><li><a href="#極致的舉一反三能力" class="table-of-contents__link toc-highlight">極致的「舉一反三」能力</a></li></ul></li><li><a href="#技術細節adapter-與-lora" class="table-of-contents__link toc-highlight">技術細節：Adapter 與 LoRA</a></li><li><a href="#微調的兩條路線打造專才-specialist" class="table-of-contents__link toc-highlight">微調的兩條路線：打造專才 (Specialist)</a><ul><li><a href="#具體實例說明" class="table-of-contents__link toc-highlight">具體實例說明</a></li><li><a href="#代表性模型與歷史背景" class="table-of-contents__link toc-highlight">代表性模型與歷史背景</a></li><li><a href="#效能表現" class="table-of-contents__link toc-highlight">效能表現</a></li></ul></li><li><a href="#微調的兩條路線打造通才-generalist" class="table-of-contents__link toc-highlight">微調的兩條路線：打造通才 (Generalist)</a><ul><li><a href="#具體實例說明-1" class="table-of-contents__link toc-highlight">具體實例說明</a></li><li><a href="#技術挑戰連續學習與遺忘" class="table-of-contents__link toc-highlight">技術挑戰：連續學習與遺忘</a></li><li><a href="#關鍵進展google-的-flan-系列" class="table-of-contents__link toc-highlight">關鍵進展：Google 的 FLAN 系列</a></li><li><a href="#openai-的突破instructgpt-與真實數據" class="table-of-contents__link toc-highlight">OpenAI 的突破：InstructGPT 與真實數據</a></li></ul></li><li><a href="#高品質資料的匱乏與-self-instruct-的誕生" class="table-of-contents__link toc-highlight">高品質資料的匱乏與 Self-Instruct 的誕生</a><ul><li><a href="#逆向工程self-instruct" class="table-of-contents__link toc-highlight">逆向工程（Self-Instruct）</a></li></ul></li><li><a href="#預訓練模型參數的壟斷與-llama-的開放" class="table-of-contents__link toc-highlight">預訓練模型參數的壟斷與 LLaMA 的開放</a><ul><li><a href="#llama-的出現與解放" class="table-of-contents__link toc-highlight">LLaMA 的出現與解放</a></li></ul></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">This Website</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/notes">Notes</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/research">Research</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://profile.104.com.tw/profile/9f11e5a7-4d3b-4a21-9241-ba6e3f9003c8/about" target="_blank" rel="noopener noreferrer" class="footer__link-item">104<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/%E7%85%A7%E6%81%A9-%E9%BB%83-93511b25b" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Acknowledgement</div><ul class="footer__items clean-list"><li class="footer__item">
              <p>
              illustrations by <a href="https://storyset.com/web">Storyset</a>
              </p>
              </li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Chao-En Huang. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>