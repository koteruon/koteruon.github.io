<!doctype html>
<html lang="zh-Hant" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notes/hung-yi-lee-2025/how-llms-perform-reasoning" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.7.0">
<title data-rh="true">DeepSeek-R1 這類大型語言模型是如何進行「深度思考」(Reasoning) 的？ | Chao-En Huang</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://blog.koteruon.com/images/icon/chao_en_huang_icon.png"><meta data-rh="true" name="twitter:image" content="https://blog.koteruon.com/images/icon/chao_en_huang_icon.png"><meta data-rh="true" property="og:url" content="https://blog.koteruon.com/docs/notes/hung-yi-lee-2025/how-llms-perform-reasoning"><meta data-rh="true" property="og:locale" content="zh_Hant"><meta data-rh="true" name="docusaurus_locale" content="zh-Hant"><meta data-rh="true" name="docsearch:language" content="zh-Hant"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="DeepSeek-R1 這類大型語言模型是如何進行「深度思考」(Reasoning) 的？ | Chao-En Huang"><meta data-rh="true" name="description" content="深度思考 (Reasoning) 的定義與原理"><meta data-rh="true" property="og:description" content="深度思考 (Reasoning) 的定義與原理"><link data-rh="true" rel="icon" href="/images/icon/favicon.ico"><link data-rh="true" rel="canonical" href="https://blog.koteruon.com/docs/notes/hung-yi-lee-2025/how-llms-perform-reasoning"><link data-rh="true" rel="alternate" href="https://blog.koteruon.com/docs/notes/hung-yi-lee-2025/how-llms-perform-reasoning" hreflang="zh-Hant"><link data-rh="true" rel="alternate" href="https://blog.koteruon.com/docs/notes/hung-yi-lee-2025/how-llms-perform-reasoning" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Chao-En Huang RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Chao-En Huang Atom Feed">

<link rel="preconnect" href="https://www.googletagmanager.com">
<script>window.dataLayer=window.dataLayer||[]</script>
<script>!function(e,t,a,n){e[n]=e[n]||[],e[n].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var g=t.getElementsByTagName(a)[0],m=t.createElement(a);m.async=!0,m.src="https://www.googletagmanager.com/gtm.js?id=GTM-5X8N2TCV",g.parentNode.insertBefore(m,g)}(window,document,"script","dataLayer")</script>





<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/assets/css/styles.42b2a4f7.css">
<script src="/assets/js/runtime~main.01fa9550.js" defer="defer"></script>
<script src="/assets/js/main.ee6b3616.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-5X8N2TCV" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>


<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const n=new URLSearchParams(window.location.search).entries();for(var[t,e]of n)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="跳至主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳至主要内容</a></div><nav aria-label="主導航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切換導覽列" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/images/icon/apple-touch-icon.png" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Chao-En</b></a><a class="navbar__item navbar__link" href="/about-me">About Me</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/notes">Notes</a><a class="navbar__item navbar__link" href="/docs/research">Research</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="切換淺色/深色模式（當前為淺色模式）" aria-label="切換淺色/深色模式（當前為淺色模式）" aria-live="polite" aria-pressed="false"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="回到頂部" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="文件側邊欄" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/docs/notes">Introduction</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/yolo/outline">YOLO 研究筆記</a><button aria-label="展開側邊欄分類 &#x27;YOLO 研究筆記&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2021/machine-learning-basics">李宏毅機器學習 2021</a><button aria-label="展開側邊欄分類 &#x27;李宏毅機器學習 2021&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/hung-yi-lee-2024/what-is-generative-ai">李宏毅生成式AI 2024</a><button aria-label="展開側邊欄分類 &#x27;李宏毅生成式AI 2024&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" href="/docs/notes/hung-yi-lee-2025/future-of-generative-ai">李宏毅生成式AI 2025</a><button aria-label="收起側邊欄分類 &#x27;李宏毅生成式AI 2025&#x27;" aria-expanded="true" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/future-of-generative-ai">生成式 AI 的技術突破與未來發展</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/principles-of-ai-agents">AI Agent 的定義與運作機制</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/inside-language-models">語言模型  內部運作機制剖析</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/is-the-transformer-era-ending">Transformer 的時代要結束了嗎？介紹 Transformer 的競爭者們</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/power-and-limits-of-pretrain-alignment">大型語言模型訓練方法「預訓練–對齊」的強大與極限</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/post-training-and-forgetting">生成式人工智慧的後訓練 (Post-Training) 與遺忘問題</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/notes/hung-yi-lee-2025/how-llms-perform-reasoning">DeepSeek-R1 這類大型語言模型是如何進行「深度思考」(Reasoning) 的？</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/reasoning-length-is-not-everything">大型語言模型的推理過程不用太長、夠用就好</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/challenges-and-myths-of-llm-evaluation">大型語言模型評估 (Evaluation) 的挑戰與迷思</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/intro-to-model-editing">人工智慧的微創手術 — 淺談 Model Editing</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/an-introduction-to-model-merging">淺談神奇的 Model Merging 技術</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/notes/hung-yi-lee-2025/how-language-models-learn-to-speak">語言模型如何學會說話 — 概述語音語言模型發展歷程</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/design-pattern">Design Patterns</a><button aria-label="展開側邊欄分類 &#x27;Design Patterns&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/docker-basics">Docker Basics</a><button aria-label="展開側邊欄分類 &#x27;Docker Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/kubernetes-basics">Kubernetes Basics</a><button aria-label="展開側邊欄分類 &#x27;Kubernetes Basics&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/owasp-top10-2021">OWASP Top 10 (2021)</a><button aria-label="展開側邊欄分類 &#x27;OWASP Top 10 (2021)&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" href="/docs/notes/ollama">Ollama</a><button aria-label="展開側邊欄分類 &#x27;Ollama&#x27;" aria-expanded="false" type="button" class="clean-btn menu__caret"></button></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="頁面路徑"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="主頁面" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/docs/notes/hung-yi-lee-2025/future-of-generative-ai"><span itemprop="name">李宏毅生成式AI 2025</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">DeepSeek-R1 這類大型語言模型是如何進行「深度思考」(Reasoning) 的？</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">本頁導覽</button></div><div class="theme-doc-markdown markdown"><header><h1>DeepSeek-R1 這類大型語言模型是如何進行「深度思考」(Reasoning) 的？</h1></header>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="深度思考-reasoning-的定義與原理">深度思考 (Reasoning) 的定義與原理<a href="#深度思考-reasoning-的定義與原理" class="hash-link" aria-label="深度思考 (Reasoning) 的定義與原理的直接連結" title="深度思考 (Reasoning) 的定義與原理的直接連結">​</a></h2>
<ul>
<li><strong>內心的小劇場</strong>：DeepSeek-R1、ChatGPT o1/o3 等模型在回答問題前，會先經歷一段長思考過程（如 <code>&lt;think&gt;</code> 標籤內的內容）。這包含驗證（Verification）、探索（Explore）與規劃（Planning），這種行為被稱為 <strong>Reasoning（推理）</strong>。</li>
<li><strong>Reasoning vs. Inference</strong>：<!-- -->
<ul>
<li><strong>Inference（推論）</strong>：指一般使用模型產生輸出的過程。</li>
<li><strong>Reasoning（推理）</strong>：特指模型在 Inference 階段產生極長的思考過程。注意：這裡的「推理」不代表跟人類思考方式一樣，僅指該行為表現。
<img decoding="async" loading="lazy" alt="reasoning" src="/assets/images/reasoning-a7fdbd95688ebe438361146e4241e2d5.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
</li>
<li><strong>Test-time Compute (測試時運算)</strong>：<!-- -->
<ul>
<li><strong>概念</strong>：在測試階段投入更多算力（思考更久），往往能得到更好的結果。這被稱為「深度不夠，長度來湊」。</li>
<li><strong>AlphaGo 的啟示</strong>：AlphaGo 除了訓練 Policy/Value Network，在下棋（Testing）時還使用了 <strong>Monte Carlo Tree Search (MCTS)</strong> 進行大量模擬運算。這證明了 <strong>Test-time Scaling</strong> 的潛力：用少量的測試運算資源，可以換取訓練階段需耗費巨量資源才能達到的效果。
<img decoding="async" loading="lazy" alt="test-time-scaling" src="/assets/images/test-time-scaling-6e531c3264a68a3bbd2392c9fa6685c6.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="打造推理語言模型的方法">打造「推理」語言模型的方法<a href="#打造推理語言模型的方法" class="hash-link" aria-label="打造「推理」語言模型的方法的直接連結" title="打造「推理」語言模型的方法的直接連結">​</a></h2>
<ul>
<li><strong>四種路徑的分類邏輯</strong>：我們將構建深度思考（Reasoning）模型的方法歸納為四大類，其核心區別在於<strong>是否需要對模型進行參數微調（Fine-tuning）</strong>。</li>
<li><strong>無需微調參數</strong>：前兩種方法（更強的思維鏈 CoT、推論工作流程）適用於現有的模型，僅透過 Prompt 引導或外部演算法介入即可提升推理能力，不需更動模型權重。</li>
<li><strong>需要微調參數</strong>：後兩種方法（Imitation Learning、Reinforcement Learning）則屬於 <strong>後訓練（Post-Training）</strong> 的範疇，需要準備特定資料來更新 Foundation Model 的參數，直接教會模型如何思考。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="training-llms-for-reasoning" src="/assets/images/training-llms-for-reasoning-2475d7398e875e9b819cacc46b451092.png" width="1280" height="720" class="img_ev3q"></p>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="方法一更強的思維鏈-chain-of-thought-cot">方法一：更強的思維鏈 (Chain-of-Thought, CoT)<a href="#方法一更強的思維鏈-chain-of-thought-cot" class="hash-link" aria-label="方法一：更強的思維鏈 (Chain-of-Thought, CoT)的直接連結" title="方法一：更強的思維鏈 (Chain-of-Thought, CoT)的直接連結">​</a></h2>
<p>此方法<strong>不需要微調參數</strong>，僅透過 Prompt 引導。</p>
<p><img decoding="async" loading="lazy" alt="cot" src="/assets/images/cot-ca4b0241be8a8703e978053b004fdcda.png" width="1280" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cot-的演進">CoT 的演進<a href="#cot-的演進" class="hash-link" aria-label="CoT 的演進的直接連結" title="CoT 的演進的直接連結">​</a></h3>
<ul>
<li><strong>Short CoT</strong>：包含 Few-shot（給範例）與 Zero-shot（&quot;Let&#x27;s think step by step&quot;）。</li>
<li><strong>Long CoT</strong>：現今推理模型產生的思考過程極長，需透過更精確的指示激發。
<img decoding="async" loading="lazy" alt="evolution-of-cot" src="/assets/images/evolution-of-cot-36eb41ca1a5f944bd8d471b3a218bf2e.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="supervised-cot">Supervised CoT<a href="#supervised-cot" class="hash-link" aria-label="Supervised CoT的直接連結" title="Supervised CoT的直接連結">​</a></h3>
<ul>
<li><strong>做法</strong>：在 Prompt 中詳細規定思考流程（如：拆解計畫、執行子計畫、多次驗算、把思考放在特定標籤內）。</li>
<li><strong>限制</strong>：只適用於較強的模型（如 GPT-4o），較弱的模型（如 Llama-3 8B）可能無法遵循複雜指令。<!-- -->
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="supervised-cot-1" src="/assets/images/supervised-cot-1-fd0886cce0873d9254e2a7c0af0f90d8.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="supervised-cot-2" src="/assets/images/supervised-cot-2-f7e1804a96736c4e3c88bdfb7e9d9209.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>透過人類知識告訴模型如何思考</strong></td><td style="text-align:center"><strong>GPT照著流程生出計畫</strong></td></tr></tbody></table>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="方法二給模型推論工作流程-inference-workflow">方法二：給模型推論工作流程 (Inference Workflow)<a href="#方法二給模型推論工作流程-inference-workflow" class="hash-link" aria-label="方法二：給模型推論工作流程 (Inference Workflow)的直接連結" title="方法二：給模型推論工作流程 (Inference Workflow)的直接連結">​</a></h2>
<p>此方法<strong>不需要微調參數</strong>，透過多次生成與演算法選出最佳解。</p>
<p><img decoding="async" loading="lazy" alt="inference-workflow" src="/assets/images/inference-workflow-5078697f6f58c3d554c8b10b234f04ea.png" width="1280" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-monkeys-無限猴子理論">Large Language Monkeys (無限猴子理論)<a href="#large-language-monkeys-無限猴子理論" class="hash-link" aria-label="Large Language Monkeys (無限猴子理論)的直接連結" title="Large Language Monkeys (無限猴子理論)的直接連結">​</a></h3>
<ul>
<li>核心概念是讓模型對同一題回答上千次，只要次數夠多，即使是較小的模型（如 1B 或 7B）也有機會「賽到」正確答案。</li>
<li>實驗顯示，只要嘗試次數（Sample counts）足夠多，模型的覆蓋率（Coverage，即至少答對一次的機率）會顯著提升，這被稱為「愚者千慮，必有一得」。
<img decoding="async" loading="lazy" alt="large-language-monkeys" src="/assets/images/large-language-monkeys-9bc2f071aaffc93af9842839045d9466.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
<hr>
<p>當模型產生了上千個答案，我們需要策略來挑出正確的那一個：</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="majority-vote-self-consistency">Majority Vote (Self-consistency)<a href="#majority-vote-self-consistency" class="hash-link" aria-label="Majority Vote (Self-consistency)的直接連結" title="Majority Vote (Self-consistency)的直接連結">​</a></h3>
<ul>
<li><strong>運作原理</strong>：這  是最簡單且強大的 Baseline。讓模型生成多次，統計出現次數最多的答案即視為正確解。</li>
<li><strong>實作技巧</strong>：為了方便統計，通常會在 Prompt 中強迫模型將答案夾在特定符號（如 <code>&lt;answer&gt;...&lt;/answer&gt;</code>）中間。</li>
<li><strong>效能表現</strong>：實驗證實，隨著生成次數增加（如從 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>1</mn></msup></mrow><annotation encoding="application/x-tex">2^1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>8</mn></msup></mrow><annotation encoding="application/x-tex">2^8</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span> 次），正確率會顯著上升。雖然單靠此方法，1B 模型仍難以超越 8B 模型，但已比原本表現好上許多。
<img decoding="async" loading="lazy" alt="majority-vote" src="/assets/images/majority-vote-b62acf2b1055c7b7787a0f6b01c172bc.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="best-of-n-verifier">Best-of-N (Verifier)<a href="#best-of-n-verifier" class="hash-link" aria-label="Best-of-N (Verifier)的直接連結" title="Best-of-N (Verifier)的直接連結">​</a></h3>
<ul>
<li><strong>運作原理</strong>：訓練一個獨立的 <strong>Verifier（驗證器）</strong> 來對模型產生的多個答案進行評分，最後選擇分數最高的那個。</li>
<li><strong>驗證器的訓練</strong>：利用現有的問題與正確答案（Ground Truth），讓模型產生多個解。若解出的答案與標準答案相符，標記為 1（正樣本）；若不符則標記為 0（負樣本）。用這些資料訓練 Verifier，讓它學會判斷哪些答案較可能是對的。</li>
<li><strong>結果</strong>：實驗顯示 Best-of-N 的效果通常優於 Majority Vote。
<img decoding="async" loading="lazy" alt="best-of-n" src="/assets/images/best-of-n-95703d271b689aed3caa1b610beae6d6.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
<hr>
<p>除了最後選答案，我們還可以控制模型生成答案的流程與中間驗證：</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="parallel-vs-sequential-並行與序列">Parallel vs. Sequential (並行與序列)<a href="#parallel-vs-sequential-並行與序列" class="hash-link" aria-label="Parallel vs. Sequential (並行與序列)的直接連結" title="Parallel vs. Sequential (並行與序列)的直接連結">​</a></h3>
<ul>
<li><strong>Parallel</strong>：同時讓模型獨立解題多次，再從中選最好的。</li>
<li><strong>Sequential</strong>：讓模型先解一次，第二次解題時參考第一次的解法（無論是對是錯）進行修正或優化，具有迭代的概念。</li>
<li><strong>混合策略</strong>：實務上可以同時結合兩者，例如先並行產生多個解  ，再針對特定解進行序列優化。<!-- -->
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="parallel-and-sequential-1" src="/assets/images/parallel-and-sequential-1-c78dac834ffab114c62685053add1249.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="parallel-and-sequential-2" src="/assets/images/parallel-and-sequential-2-a1a3e1e2e1a6fd965a1d03993a4d6ae7.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>推論流程可以是並行或序列</strong></td><td style="text-align:center"><strong>實務上可以同時結合兩者</strong></td></tr></tbody></table>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="中間步驟驗證-process-verifier">中間步驟驗證 (Process Verifier)<a href="#中間步驟驗證-process-verifier" class="hash-link" aria-label="中間步驟驗證 (Process Verifier)的直接連結" title="中間步驟驗證 (Process Verifier)的直接連結">​</a></h3>
<ul>
<li><strong>原理</strong>：不等到最後才驗證答案，而是在每個步驟（Step）結束時就進行驗證。若第一步就算錯，就直接停止該路徑，節省算力。
<img decoding="async" loading="lazy" alt="process-verifier-1" src="/assets/images/process-verifier-1-c89da772f955c348cb19d8b93a3c55b3.png" width="1280" height="720" class="img_ev3q"></li>
<li><strong>訓練方式</strong>：透過蒙地卡羅（Monte Carlo）模擬，讓模型從某一步驟開始多次解題，統計最終能答對的機率，以此作為該步驟的分數來訓練 Process Verifier。<!-- -->
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="process-verifier-2" src="/assets/images/process-verifier-2-912aa2823e35dfbab5bd410bdbbe6d38.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="process-verifier-3" src="/assets/images/process-verifier-3-2911a4e3d3fbb214978e6736a086e847.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>統計最終能答對的機率</strong></td><td style="text-align:center"><strong>將機率直接當作分數</strong></td></tr></tbody></table>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tree-search-樹狀搜尋-與-beam-search">Tree Search (樹狀搜尋) 與 Beam Search<a href="#tree-search-樹狀搜尋-與-beam-search" class="hash-link" aria-label="Tree Search (樹狀搜尋) 與 Beam Search的直接連結" title="Tree Search (樹狀搜尋) 與 Beam Search的直接連結">​</a></h3>
<ul>
<li><strong>運作機制</strong>：結合 <strong>Beam Search</strong> 或 <strong>Bin Search</strong> 的概念。在生成每一個步驟（Step）後，利用 Process Verifier 評分，每次只保留最好的 N 條路徑（例如前 25% 或分數最高的 2 條）繼續往下生成，將算力集中在最有希望的路徑上。</li>
<li><strong>越級挑戰</strong>：實驗驚人地發現，<strong>1B 的小模型搭配 Bin Search 這種推論工作流程，其解題正確率甚至能超越 8B 的大模型</strong>。這證明了只要懂得「如何思考」（Workflow），小模型也能透過演算法戰勝大模型。</li>
<li><strong>多樣化變形</strong>：除了 Bin Search，還有 A*、Monte Carlo Tree Search (MCTS) 等多種啟發式搜尋演算法（Heuristic Search）可應用於此框架中。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="beam-search" src="/assets/images/beam-search-1c795b943bcd7f59c46e94df905988da.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="heuristic-search-algorithm" src="/assets/images/heuristic-search-algorithm-36f8adb4bded4be02f40751b574e7eac.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Beam Search 只保留最好的N條路徑繼續搜尋</strong></td><td style="text-align:center"><strong>更多的啟發式搜尋演算法應用到路徑搜尋</strong></td></tr></tbody></table>
<hr>
<p><img decoding="async" loading="lazy" alt="post-training" src="/assets/images/post-training-da62f4f3964ac6473530ae3eda3e44d1.png" width="1280" height="720" class="img_ev3q"></p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="方法三教模型推理過程-imitation-learning">方法三：教模型推理過程 (Imitation Learning)<a href="#方法三教模型推理過程-imitation-learning" class="hash-link" aria-label="方法三：教模型推理過程 (Imitation Learning)的直接連結" title="方法三：教模型推理過程 (Imitation Learning)的直接連結">​</a></h2>
<p>此方法<strong>需要微調參數</strong>，直接教模型模仿推理過程。</p>
<p><img decoding="async" loading="lazy" alt="imitation-learning" src="/assets/images/imitation-learning-7d2a5f4243d3f09d1cbd31b2279b692b.png" width="1280" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="想辦法生成推論過程的訓練資料-self-output">想辦法生成推論過程的訓練資料 (Self-output)<a href="#想辦法生成推論過程的訓練資料-self-output" class="hash-link" aria-label="想辦法生成推論過程的訓練資料 (Self-output)的直接連結" title="想辦法生成推論過程的訓練資料 (Self-output)的直接連結">​</a></h3>
<ul>
<li><strong>難點與解法</strong>：一般的訓練資料通常只有「問題」與「正確答案」，缺乏中間的推論過程。解決方案是讓語言模型自己生成 CoT（Chain of Thought），若最終答案與標準答案（Ground Truth）相符，就假設其中間的推理過程也是正確的，並將其作為訓練資料。</li>
<li><strong>驗證機制</strong>：除了比對答案，也可以引入一個 <strong>Verifier（驗證器）</strong> 來判斷答案的正確性。若 Verifier 認為答案是對的，就採納 該推理過程。</li>
</ul>
<p><img decoding="async" loading="lazy" alt="generating-reasoning-traces" src="/assets/images/generating-reasoning-traces-69475ae74eaa53db71f60aaa585e01e6.png" width="1280" height="720" class="img_ev3q"></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rstar-math-確保步驟正確性">rStar-Math (確保步驟正確性)<a href="#rstar-math-確保步驟正確性" class="hash-link" aria-label="rStar-Math (確保步驟正確性)的直接連結" title="rStar-Math (確保步驟正確性)的直接連結">​</a></h3>
<ul>
<li><strong>解決「幸運猜對」的問題</strong>：單純的 Self-output 無法保證「答案對，過程就一定對」（可能只是賽到的）。<strong>rStar-Math</strong> 透過展開樹狀搜尋（Tree Search），並使用 <strong>Process Verifier</strong> 對<strong>每一個步驟</strong>進行驗證。</li>
<li><strong>高品質資料生成</strong>：只有當中間步驟被 Process Verifier 認為是合理的，且最終導向正確答案時，這整串路徑才會被合併成高品質的 Reasoning Process 資料給模型學習。這能確保模型學到的是邏輯嚴謹的推論，而非錯誤的歸因。</li>
<li><strong>多樣化的訓練策略 (SFT &amp; RL)</strong>：<!-- -->
<ul>
<li><strong>Supervised Fine-tuning (Imitation Learning)</strong>：將經由樹狀搜尋驗證過的正確推論路徑視為標準答案，直接進行監督式微調，教模型模仿正確的思考過程。</li>
<li><strong>Reinforcement Learning (RL)</strong>：除了單純模仿，也可利用樹狀結構中的路徑差異（正確 vs. 錯誤路徑），告訴模型「哪一步是好的（增加機率）」、「哪一步是不好的（降低機率）」，類似 DPO 的做法。</li>
</ul>
</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="rstar-math-1" src="/assets/images/rstar-math-1-1a64f7686b2fb44c2e8c31e812f93589.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="rstar-math-2" src="/assets/images/rstar-math-2-e750b5e4a591a75017deef0a36f28c90.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>正確結果的路徑 Reasoning Process 資料給模型學習</strong></td><td style="text-align:center"><strong>不一定要Supervised Fine-tuning，RL也可以</strong></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="stream-of-search-與-journey-learning-學習知錯能改">Stream of Search 與 Journey Learning (學習知錯能改)<a href="#stream-of-search-與-journey-learning-學習知錯能改" class="hash-link" aria-label="Stream of Search 與 Journey Learning (學習知錯能改)的直接連結" title="Stream of Search 與 Journey Learning (學習知錯能改)的直接連結">​</a></h3>
<ul>
<li><strong>Shortcut Learning 的陷阱</strong>：若只給模型看「每一步都完美」的推論過程（Shortcut Learning），模型會變成只能打順風局。一旦中間出錯，模型因為沒看過錯誤，會不知道如何修正，甚至會為了湊出答案而硬凹（Hallucination）。</li>
<li><strong>Journey Learning (逆轉勝)</strong>：為了教導模型「知錯能改」，訓練資料必須包含「走錯路、發現錯誤、修正回來」的過程。<!-- -->
<ul>
<li><strong>Stream of Search</strong>：此方法會在訓練資料中故意保留錯誤的路徑（例如：先走錯的 Step 2，收到 Verifier 回饋說錯了，再退回重走正確的 Step 2）。</li>
<li><strong>加入回饋與連接詞</strong>：為了讓語言邏輯通順，會在錯誤與修正之間插入 Verifier 的回饋（如：「這個解法有錯，讓我們重新來過」）作為連接詞。實驗證明，這種包含錯誤修正歷程的 Journey Learning，其訓練效果優於只學完美路徑。</li>
</ul>
</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="stream-of-search-1" src="/assets/images/stream-of-search-1-f8682b4620e9c49252b7cd5021298f06.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="stream-of-search-2" src="/assets/images/stream-of-search-2-42d4b41e036252e7f8a1e01a03a5acdb.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>只給正確的推理過程，模型不知道找自己的問題</strong></td><td style="text-align:center"><strong>增加 Verifier 的回饋，可以幫助人讀懂邏輯順序</strong></td></tr></tbody></table>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="knowledge-distillation-知識蒸餾">Knowledge Distillation (知識蒸餾)<a href="#knowledge-distillation-知識蒸餾" class="hash-link" aria-label="Knowledge Distillation (知識蒸餾)的直接連結" title="Knowledge Distillation (知識蒸餾)的直接連結">​</a></h3>
<ul>
<li><strong>站在巨人的肩膀上</strong>：既然市面上已有強大的推理模型（如 DeepSeek-R1），我們可以拿它當老師。直接將問題輸入給強模型，獲取其生成的完整推理過程，再拿這些資料來訓練較小的模型（如 Llama 或 Qwen）。</li>
<li><strong>小模型的救星</strong>：DeepSeek 的技術報告顯示，對於像 Qwen-32B 這類中小型模型，單純用強化學習（RL）很難激發其能力；但若先透過 Distillation 向 DeepSeek-R1 學習推理過程，能力就會大幅提升，甚至在數學與程式任務上能與大模型相提並論。</li>
</ul>
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="knowledge-distillation-1" src="/assets/images/knowledge-distillation-1-361bd6770d29812a8f387d5361851ebb.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="knowledge-distillation-2" src="/assets/images/knowledge-distillation-2-52269d6c73447fc17614bf8aad09a44b.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>現在就可以做 Knowledge Distillation</strong></td><td style="text-align:center"><strong>實驗結果</strong></td></tr></tbody></table>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="方法四以結果為導向學習推理-result-oriented-rl">方法四：以結果為導向學習推理 (Result-Oriented RL)<a href="#方法四以結果為導向學習推理-result-oriented-rl" class="hash-link" aria-label="方法四：以結果為導向學習推理 (Result-Oriented RL)的直接連結" title="方法四：以結果為導向學習推理 (Result-Oriented RL)的直接連結">​</a></h2>
<p>此方法<strong>需要微調參數</strong>，使用強化學習（RL），只看結果不問過程。</p>
<p><img decoding="async" loading="lazy" alt="rl" src="/assets/images/rl-40a1ab5becdcdd2f0d01d7b0ea9b212d.png" width="1280" height="720" class="img_ev3q"></p>
<ul>
<li><strong>運算邏輯</strong>：只要答案正確就給 Positive Reward，答案錯誤給 Negative Reward，完全不在意中間推論寫了什麼。</li>
<li><strong>DeepSeek-R1-Zero (純 RL 版本)</strong>：<!-- -->
<ul>
<li><strong>Aha Moment (頓悟)</strong>：模型在純 RL 訓練過程中，自發性學會了檢查錯誤、重新思考的行為，這不是人類教的，而是模型自己學到的。</li>
<li><strong>缺點</strong>：推論過程極難閱讀，語言混雜，因此未正式發布。
<img decoding="async" loading="lazy" alt="result-oriented-rl" src="/assets/images/result-oriented-rl-41b3abbeb365ce4f1f29d74bbf4067f7.png" width="1280" height="720" class="img_ev3q"></li>
</ul>
</li>
<li><strong>DeepSeek-R1 (正式版) 的四階段訓練</strong>：<!-- -->
<ol>
<li><strong>Cold Start</strong>：利用少量人工介入或 Prompt 生成的高品質推理資料（解決 R1-Zero 語言混亂問題），訓練出 Model A。</li>
<li><strong>Reasoning RL</strong>：對 Model A 進行 RL 訓練（加入語言一致性 Reward），得到 Model B。</li>
<li><strong>Rejection Sampling &amp; SFT</strong>：用 Model B 生成大量資料（60萬筆），並過濾掉爛的，重新做 Imitation Learning 得到 Model C。此階段會加入非推理任務資料以維持通用能力。</li>
<li><strong>All-round RL</strong>：最後進行針對安全性（Safety）與實用性（Helpfulness）的 RL，完成 DeepSeek-R1。<!-- -->
<table><thead><tr><th style="text-align:center"><img decoding="async" loading="lazy" alt="deepseek-r1-1" src="/assets/images/deepseek-r1-1-687a30193e7e645ca7e700ec96b25b3e.png" width="1280" height="720" class="img_ev3q"></th><th style="text-align:center"><img decoding="async" loading="lazy" alt="deepseek-r1-2" src="/assets/images/deepseek-r1-2-fdc3ae68de2c71145b0dfdb86197d78c.png" width="1280" height="720" class="img_ev3q"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>步驟1、2</strong></td><td style="text-align:center"><strong>步驟3、4</strong></td></tr></tbody></table>
</li>
</ol>
</li>
</ul>
<hr>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="推理能力是被喚醒的不是被訓練出來的">推理能力是被喚醒的，不是被訓練出來的<a href="#推理能力是被喚醒的不是被訓練出來的" class="hash-link" aria-label="推理能力是被喚醒的，不是被訓練出來的的直接連結" title="推理能力是被喚醒的，不是被訓練出來的的直接連結">​</a></h2>
<ul>
<li><strong>RL 是強化「既有」能力</strong>：RL 能夠成功的關鍵在於 Foundation Model 本身就具備推理的潛力（只是沒被激發）。如果是很弱的模型（如 Qwen-32B Base），純用 RL 效果有限；但若先透過 Imitation Learning（向 R1 學習）再做 RL，效果就會非常好。
<img decoding="async" loading="lazy" alt="rl-amplifies-existing-capabilities" src="/assets/images/rl-amplifies-existing-capabilities-a3d54de6bc9ae70022ae9b459664fe7c.png" width="1280" height="720" class="img_ev3q"></li>
<li><strong>方法不互斥</strong>：上述四種方法可以疊加。例如 DeepSeek-R1 在訓練時用了 Imitation 和 RL，在測試時（Inference）依然可以用 Majority Vote 進一步提升效能。</li>
</ul></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="文件選項卡"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/notes/hung-yi-lee-2025/post-training-and-forgetting"><div class="pagination-nav__sublabel">上一頁</div><div class="pagination-nav__label">生成式人工智慧的後訓練 (Post-Training) 與遺忘問題</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/notes/hung-yi-lee-2025/reasoning-length-is-not-everything"><div class="pagination-nav__sublabel">下一頁</div><div class="pagination-nav__label">大型語言模型的推理過程不用太長、夠用就好</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#深度思考-reasoning-的定義與原理" class="table-of-contents__link toc-highlight">深度思考 (Reasoning) 的定義與原理</a></li><li><a href="#打造推理語言模型的方法" class="table-of-contents__link toc-highlight">打造「推理」語言模型的方法</a></li><li><a href="#方法一更強的思維鏈-chain-of-thought-cot" class="table-of-contents__link toc-highlight">方法一：更強的思維鏈 (Chain-of-Thought, CoT)</a><ul><li><a href="#cot-的演進" class="table-of-contents__link toc-highlight">CoT 的演進</a></li><li><a href="#supervised-cot" class="table-of-contents__link toc-highlight">Supervised CoT</a></li></ul></li><li><a href="#方法二給模型推論工作流程-inference-workflow" class="table-of-contents__link toc-highlight">方法二：給模型推論工作流程 (Inference Workflow)</a><ul><li><a href="#large-language-monkeys-無限猴子理論" class="table-of-contents__link toc-highlight">Large Language Monkeys (無限猴子理論)</a></li><li><a href="#majority-vote-self-consistency" class="table-of-contents__link toc-highlight">Majority Vote (Self-consistency)</a></li><li><a href="#best-of-n-verifier" class="table-of-contents__link toc-highlight">Best-of-N (Verifier)</a></li><li><a href="#parallel-vs-sequential-並行與序列" class="table-of-contents__link toc-highlight">Parallel vs. Sequential (並行與序列)</a></li><li><a href="#中間步驟驗證-process-verifier" class="table-of-contents__link toc-highlight">中間步驟驗證 (Process Verifier)</a></li><li><a href="#tree-search-樹狀搜尋-與-beam-search" class="table-of-contents__link toc-highlight">Tree Search (樹狀搜尋) 與 Beam Search</a></li></ul></li><li><a href="#方法三教模型推理過程-imitation-learning" class="table-of-contents__link toc-highlight">方法三：教模型推理過程 (Imitation Learning)</a><ul><li><a href="#想辦法生成推論過程的訓練資料-self-output" class="table-of-contents__link toc-highlight">想辦法生成推論過程的訓練資料 (Self-output)</a></li><li><a href="#rstar-math-確保步驟正確性" class="table-of-contents__link toc-highlight">rStar-Math (確保步驟正確性)</a></li><li><a href="#stream-of-search-與-journey-learning-學習知錯能改" class="table-of-contents__link toc-highlight">Stream of Search 與 Journey Learning (學習知錯能改)</a></li><li><a href="#knowledge-distillation-知識蒸餾" class="table-of-contents__link toc-highlight">Knowledge Distillation (知識蒸餾)</a></li></ul></li><li><a href="#方法四以結果為導向學習推理-result-oriented-rl" class="table-of-contents__link toc-highlight">方法四：以結果為導向學習推理 (Result-Oriented RL)</a></li><li><a href="#推理能力是被喚醒的不是被訓練出來的" class="table-of-contents__link toc-highlight">推理能力是被喚醒的，不是被訓練出來的</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">This Website</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/docs/notes">Notes</a></li><li class="footer__item"><a class="footer__link-item" href="/docs/research">Research</a></li><li class="footer__item"><a class="footer__link-item" href="/blog">Blog</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/koteruon" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://profile.104.com.tw/profile/9f11e5a7-4d3b-4a21-9241-ba6e3f9003c8/about" target="_blank" rel="noopener noreferrer" class="footer__link-item">104<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.linkedin.com/in/%E7%85%A7%E6%81%A9-%E9%BB%83-93511b25b" target="_blank" rel="noopener noreferrer" class="footer__link-item">Linkedin<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Acknowledgement</div><ul class="footer__items clean-list"><li class="footer__item">
              <p>
              illustrations by <a href="https://storyset.com/web">Storyset</a>
              </p>
              </li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2026 Chao-En Huang. Built with Docusaurus.</div></div></div></footer></div>
</body>
</html>